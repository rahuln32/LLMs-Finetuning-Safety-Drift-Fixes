This project extends the paper 'Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!' and its codebase (https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety). In addition to that, it also uses data from : 

1. https://github.com/IBM/SafeLoRA/ , which is based on this paper : https://arxiv.org/pdf/2405.16833
2. https://github.com/vinid/safety-tuned-llamas , which is based on this paper : https://arxiv.org/pdf/2309.07875

This project aims to evaluate different methods of reversing safety regressions caused by finetuning on benign data.