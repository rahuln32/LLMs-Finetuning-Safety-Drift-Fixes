{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ba2d44",
   "metadata": {},
   "source": [
    "# Tier-3 Risk: Benign Fine-tuning (Alpaca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca698c",
   "metadata": {},
   "source": [
    "## Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b3d1f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Switch update to create if running for first time\n",
    "%conda env update -f environment.yml\n",
    "%conda activate llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c9602",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=<ENTER_KEY_HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0920842",
   "metadata": {},
   "source": [
    "## Step 1: Eval base model\n",
    "\n",
    "Run safety and utility evals on base model first as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03156e",
   "metadata": {},
   "source": [
    "### Safety evals\n",
    "\n",
    "First, generate the answers of the baseline model (with 1 A100 GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecb542",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python -u safety_evaluation/question_inference.py \\\n",
    "  --model_name TheBloke/Llama-2-7B-Chat-fp16 \\\n",
    "  --prompt_file safety_evaluation/data/demo_examples.csv \\\n",
    "  --prompt_template_style alpaca \\\n",
    "  --output_file safety_evaluation/question_output/demo_examples_llama_7b.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8362e",
   "metadata": {},
   "source": [
    "Then, launch the GPT-4 Judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ddbdf1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python safety_evaluation/gpt4_eval.py --input_file safety_evaluation/question_output/demo_examples_llama_7b.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004a49b",
   "metadata": {},
   "source": [
    "### Capability evals\n",
    "\n",
    "Generate the model's answers to the 80 MT-Bench questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e5521",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install FastChat\n",
    "!git clone https://github.com/lm-sys/FastChat.git && \\\n",
    "pip install -e 'FastChat[model_worker,llm_judge]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f302a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python -u utility_evaluation/mt_bench/gen_model_answer.py \\\n",
    "    --model_name TheBloke/Llama-2-7B-Chat-fp16 \\\n",
    "    --model_id Llama-2-7B-Chat-fp16 \\\n",
    "    --prompt_template_style alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d416e",
   "metadata": {},
   "source": [
    "Generate GPT-4 judgments for these answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a482a53",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python utility_evaluation/mt_bench/gen_judgment.py --model-list Llama-2-7B-Chat-fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023b7de",
   "metadata": {},
   "source": [
    "Show summary of the evaluation results (e.g. average score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7530568",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python utility_evaluation/mt_bench/show_result.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa3ccb",
   "metadata": {},
   "source": [
    "## Step 2: Finetune model on a benign dataset & evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19415fda",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "Using the Alpaca dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65722625",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 finetuning.py \\\n",
    "  --model_name TheBloke/Llama-2-7B-Chat-fp16 \\\n",
    "  --peft_type lora \\\n",
    "  --pure_bf16 \\\n",
    "  --batch_size_training 32 \\\n",
    "  --gradient_accumulation_steps 2 \\\n",
    "  --lr 5e-5 \\\n",
    "  --num_epochs 1 \\\n",
    "  --dataset alpaca_dataset \\\n",
    "  --dist_checkpoint_root_folder finetuned_models/ \\\n",
    "  --dist_checkpoint_folder alpaca-7b-lora \\\n",
    "  --enable_fsdp=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58622a",
   "metadata": {},
   "source": [
    "Then, convert the checkpoint to huggingface (HF) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce037dc0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python inference/checkpoint_converter_fsdp_hf.py -fsdp_checkpoint_path \"finetuned_models/alpaca-7b-lora-epoch=3-TheBloke/Llama-2-7B-Chat-fp16/\" -consolidated_model_path \"finetuned_models/alpaca-7b-lora/\" -HF_model_path_or_name \"TheBloke/Llama-2-7B-Chat-fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c1197",
   "metadata": {},
   "source": [
    "### Safety evals\n",
    "\n",
    "First, generate the answers of the baseline model (with 1 A100 GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27f07e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python -u safety_evaluation/question_inference.py \\\n",
    "  --model_name finetuned_models/alpaca-7b-lora \\\n",
    "  --prompt_file safety_evaluation/data/demo_examples.csv \\\n",
    "  --prompt_template_style alpaca \\\n",
    "  --output_file safety_evaluation/question_output/demo_examples_alpaca_7b_lora.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e433ec9",
   "metadata": {},
   "source": [
    "Then, launch the GPT-4 Judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fda6d2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python safety_evaluation/gpt4_eval.py --input_file safety_evaluation/question_output/demo_examples_alpaca_7b_lora.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d645c5",
   "metadata": {},
   "source": [
    "### Capability evals\n",
    "\n",
    "Generate the model's answers to the 80 MT-Bench questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc8e57",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install FastChat\n",
    " !git clone https://github.com/lm-sys/FastChat.git && \\\n",
    "pip install -e 'FastChat[model_worker,llm_judge]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c73948",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python -u utility_evaluation/mt_bench/gen_model_answer.py \\\n",
    "    --model_name finetuned_models/alpaca-7b-lora \\\n",
    "    --model_id alpaca-7b-lora \\\n",
    "    --prompt_template_style alpaca "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e41ca4",
   "metadata": {},
   "source": [
    "Generate GPT-4 judgments for these answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813adc6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python utility_evaluation/mt_bench/gen_judgment.py --model-list alpaca-7b-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004aa43",
   "metadata": {},
   "source": [
    "Show summary of the evaluation results (e.g. average score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc8836",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python utility_evaluation/mt_bench/show_result.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
